{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\nHello from the pygame community. https://www.pygame.org/contribute.html\ncouldn't import doomish\nCouldn't import doom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_ple\n",
    "\n",
    "from ple import PLE\n",
    "from ple.games.waterworld import WaterWorld\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, \\\n",
    "    Convolution2D, Permute, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import model_from_config\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'WaterWorld-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "INPUT_SHAPE = (FRAME_WIDTH, FRAME_HEIGHT)\n",
    "WINDOW_LENGTH = 4  # Number of most recent frames to produce the input to the network (WINDOW LENGTH)\n",
    "\n",
    "NUM_STEPS = 4_000_005\n",
    "\n",
    "EXPLORATION_STEPS = 2_000_005  # Number of steps over which the initial value # of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "\n",
    "OBSERVE = 20_000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400_000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10_000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by optimizer\n",
    "GAMMA = 0.99  # Discount factor\n",
    "\n",
    "SAVE_INTERVAL = 200_000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 7  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\nNo. of actions:  5\n"
     ]
    }
   ],
   "source": [
    "env = get_env()\n",
    "x_t = env.reset()\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "print('No. of actions: ', nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE:  (4, 84, 84)\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\npermute_1 (Permute)          (None, 84, 84, 4)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 20, 20, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 9, 9, 64)          0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               1606144   \n_________________________________________________________________\nactivation_4 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 2565      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 1,686,693\nTrainable params: 1,686,693\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE:  (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "\n",
    "def r(raw):\n",
    "    raw = [f.reshape((1,) + INPUT_SHAPE) for f in raw]\n",
    "    raw = np.asarray(raw).reshape(input_shape)\n",
    "    raw = np.expand_dims(raw, axis=0)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def process_observation(observation):\n",
    "    assert observation.ndim == 3  # (height, width, channel)\n",
    "    img = Image.fromarray(observation)\n",
    "    img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "    processed_observation = np.array(img)\n",
    "    assert processed_observation.shape == INPUT_SHAPE\n",
    "    return processed_observation.astype('uint8') / 255.  # saves storage in experience memory\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        # (width, height, channels)\n",
    "        model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "    elif K.image_dim_ordering() == 'th':\n",
    "        # (channels, width, height)\n",
    "        model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "    else:\n",
    "        raise RuntimeError('Unknown image_dim_ordering.')\n",
    "    print('INPUT SHAPE: ', input_shape)\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.compile(loss=huber_loss, optimizer=Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "\n",
    "target_model = get_model()\n",
    "target_model.compile(loss=huber_loss, optimizer=Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "\n",
    "\n",
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(env.action_space.n)\n",
    "    if random.random() < 0.5:\n",
    "        act_values = model.predict(r(state))\n",
    "    else:\n",
    "        act_values = target_model.predict(r(state))\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "def get_test_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(env.action_space.n)\n",
    "    act_values = model.predict(r(state))\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "def update_model():\n",
    "    minibatch = random.sample(memory, BATCH_SIZE)\n",
    "    total_loss = 0.\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            best_action = np.argmax(model.predict(r(next_state))[0])\n",
    "            target = (reward + GAMMA * target_model.predict(r(next_state))[0][best_action])\n",
    "        target_f = model.predict(r(state))\n",
    "        target_f[0][action] = target\n",
    "        h = model.fit(r(state), target_f, epochs=1, verbose=0)\n",
    "        total_loss += h.history['loss'][0]\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def update_target():\n",
    "    minibatch = random.sample(memory, BATCH_SIZE)\n",
    "    total_loss = 0.\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            best_action = np.argmax(target_model.predict(r(next_state))[0])\n",
    "            target = (reward + GAMMA * model.predict(r(next_state))[0][best_action])\n",
    "        target_f = target_model.predict(r(state))\n",
    "        target_f[0][action] = target\n",
    "        h = model.fit(r(state), target_f, epochs=1, verbose=0)\n",
    "        total_loss += h.history['loss'][0]\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def replay():\n",
    "    if random.random() > 0.5:\n",
    "        return update_target()\n",
    "    else:\n",
    "        return update_model()\n",
    "\n",
    "\n",
    "def setup_summary():\n",
    "    episode_total_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Total_Reward/Episode', episode_total_reward)\n",
    "    episode_avg_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Average_Max_Q/Episode', episode_avg_max_q)\n",
    "    episode_duration = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n",
    "    episode_avg_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Average_Loss/Episode', episode_avg_loss)\n",
    "    epsilon = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Epsilon/Episode', epsilon)\n",
    "    curr_time_step = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Steps/Episode', curr_time_step)\n",
    "\n",
    "    summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss, epsilon,\n",
    "                    curr_time_step]\n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return summary_placeholders, update_ops, summary_op\n",
    "# def get_initial_state(self, observation):\n",
    "#     processed_observation = np.uint8(resize(rgb2gray(observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "#     state = [processed_observation for _ in range(WINDOW_LENGTH)]\n",
    "#     return np.stack(state, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ankur/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/6000005 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 1/6000005 [00:00<300:30:00,  5.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 14/6000005 [00:00<214:18:08,  7.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 32/6000005 [00:00<152:55:34, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      1 / TIMESTEP:        3 / DURATION:     4 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 1.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 45/6000005 [00:00<110:55:43, 15.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 62/6000005 [00:00<80:42:59, 20.65it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 78/6000005 [00:00<59:42:27, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 95/6000005 [00:00<44:49:12, 37.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 111/6000005 [00:00<34:35:34, 48.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 126/6000005 [00:01<28:30:41, 58.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 140/6000005 [00:01<23:35:12, 70.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      2 / TIMESTEP:      125 / DURATION:   122 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 1.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 158/6000005 [00:01<19:31:56, 85.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 174/6000005 [00:01<17:01:05, 97.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 189/6000005 [00:01<15:32:28, 107.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 205/6000005 [00:01<14:04:20, 118.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 222/6000005 [00:01<12:50:44, 129.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 241/6000005 [00:01<11:42:33, 142.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 259/6000005 [00:01<11:02:48, 150.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 276/6000005 [00:01<10:42:09, 155.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 293/6000005 [00:02<10:46:28, 154.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 310/6000005 [00:02<10:40:27, 156.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 327/6000005 [00:02<10:59:42, 151.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 343/6000005 [00:02<11:39:40, 142.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 358/6000005 [00:02<12:29:17, 133.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 372/6000005 [00:02<12:34:27, 132.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 386/6000005 [00:02<12:45:22, 130.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 400/6000005 [00:02<12:59:09, 128.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 415/6000005 [00:02<12:36:45, 132.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 431/6000005 [00:03<12:05:18, 137.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 446/6000005 [00:03<11:52:37, 140.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 461/6000005 [00:03<11:40:25, 142.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 479/6000005 [00:03<10:57:50, 152.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      3 / TIMESTEP:      447 / DURATION:   322 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 1.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:      4 / TIMESTEP:      451 / DURATION:     4 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 1.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 497/6000005 [00:03<10:27:37, 159.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 515/6000005 [00:03<10:10:44, 163.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 532/6000005 [00:03<10:17:27, 161.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 549/6000005 [00:03<10:21:35, 160.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      5 / TIMESTEP:      533 / DURATION:    82 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 1.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 566/6000005 [00:03<11:27:23, 145.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 581/6000005 [00:04<11:38:54, 143.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 602/6000005 [00:04<10:32:59, 157.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 623/6000005 [00:04<9:50:34, 169.31it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3be6e50bc273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mduration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtotal_q_max\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "memory = deque(maxlen=NUM_REPLAY_MEMORY)\n",
    "state = deque(maxlen=WINDOW_LENGTH)\n",
    "\n",
    "env = get_env()\n",
    "observation = env.reset()\n",
    "observation = process_observation(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "\n",
    "current = [observation for _ in range(WINDOW_LENGTH)]\n",
    "previous = [observation for _ in range(WINDOW_LENGTH)]\n",
    "\n",
    "# Parameters used for summary\n",
    "total_reward = 0.\n",
    "total_q_max = 0.\n",
    "total_loss = 0.\n",
    "duration = 0\n",
    "episode = 0\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "summary_placeholders, update_ops, summary_op = setup_summary()\n",
    "summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, sess.graph)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "for t in tqdm(range(NUM_STEPS)):\n",
    "    action = get_action(current, epsilon)\n",
    "\n",
    "    observation, reward, terminal, _ = env.step(int(action))\n",
    "    observation = process_observation(observation)\n",
    "    total_reward += reward\n",
    "    duration += 1\n",
    "\n",
    "    total_q_max += np.argmax(model.predict(r(state)))\n",
    "    state.append(observation)\n",
    "\n",
    "    # Remember\n",
    "    current = list(state)\n",
    "    memory.append((previous, action, reward, current, terminal))\n",
    "    previous = current\n",
    "\n",
    "    if t > OBSERVE:\n",
    "        # Train\n",
    "        if t % TRAIN_INTERVAL == 0:\n",
    "            total_loss += replay()\n",
    "        \n",
    "        # Save Weights Interval\n",
    "        if t % SAVE_INTERVAL == 0:\n",
    "            model.save_weights((SAVE_NETWORK_PATH + '/ww_{}.h5').format(t))\n",
    "    \n",
    "        # Update Target Network\n",
    "        if t % TARGET_UPDATE_INTERVAL == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "        \n",
    "        # Exploration Rate Decay\n",
    "        if epsilon > FINAL_EPSILON:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "\n",
    "    # End of episode\n",
    "    if terminal:\n",
    "        # Writing Summary Log\n",
    "        avg_q = total_q_max / float(duration)\n",
    "        avg_loss = total_loss / (float(duration) / float(TRAIN_INTERVAL))\n",
    "        if t >= OBSERVE:\n",
    "            stats = [total_reward, avg_q, duration, avg_loss, float(epsilon), float(t)]\n",
    "\n",
    "            for i in range(len(stats)):\n",
    "                sess.run(update_ops[i], feed_dict={\n",
    "                    summary_placeholders[i]: float(stats[i])\n",
    "                })\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, episode + 1)\n",
    "\n",
    "        # Debug\n",
    "        if t < OBSERVE:\n",
    "            mode = 'random'\n",
    "        elif OBSERVE <= t < OBSERVE + EXPLORATION_STEPS:\n",
    "            mode = 'explore'\n",
    "        else:\n",
    "            mode = 'exploit'\n",
    "        print(\n",
    "            'EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} /'\n",
    "            ' EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / '\n",
    "            'AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                episode + 1, t, duration, epsilon,\n",
    "                total_reward, avg_q, avg_loss, mode))\n",
    "\n",
    "        total_reward = 0.\n",
    "        total_q_max = 0.\n",
    "        total_loss = 0.\n",
    "        duration = 0\n",
    "        episode += 1\n",
    "\n",
    "        observation = env.reset()\n",
    "        for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "            observation, _, terminal, _ = env.step(env.action_space.sample())  # Do nothing\n",
    "            if terminal:\n",
    "                observation = env.reset()\n",
    "                break\n",
    "        observation = process_observation(observation)\n",
    "        state = deque(maxlen=WINDOW_LENGTH)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        current = [observation for i in range(4)]\n",
    "        previous = [observation for i in range(4)]\n",
    "\n",
    "model.save_weights(SAVE_NETWORK_PATH + '/ww_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 1/10 [00:01<00:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 2/10 [00:01<00:08,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 3/10 [00:01<00:05,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 96\n2.0 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 4/10 [00:02<00:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 5/10 [00:04<00:05,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 6/10 [00:05<00:04,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 7/10 [00:06<00:02,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 8/10 [00:06<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 9/10 [00:07<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 10/10 [00:08<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "for e in tqdm(range(10)):\n",
    "    state = deque(maxlen=WINDOW_LENGTH)\n",
    "    env = get_env()\n",
    "    observation = env.reset()\n",
    "    observation = process_observation(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "\n",
    "    current = [observation for _ in range(WINDOW_LENGTH)]\n",
    "\n",
    "    total_reward = 0.\n",
    "    duration = 0\n",
    "    episode = 0\n",
    "\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = get_test_action(current, 0.)\n",
    "        # env.render()\n",
    "        observation, reward, terminal, _ = env.step(int(action))\n",
    "        observation = process_observation(observation)\n",
    "        total_reward += reward\n",
    "        duration += 1\n",
    "\n",
    "        state.append(observation)\n",
    "\n",
    "        # Remember\n",
    "        current = list(state)\n",
    "\n",
    "        if duration >= 600:\n",
    "            print(duration)\n",
    "            break\n",
    "\n",
    "    print(total_reward, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
