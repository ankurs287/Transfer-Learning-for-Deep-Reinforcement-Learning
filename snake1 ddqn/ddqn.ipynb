{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_snake\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, \\\n",
    "    Convolution2D, Permute, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import model_from_config\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'snake-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "INPUT_SHAPE = (FRAME_WIDTH, FRAME_HEIGHT)\n",
    "WINDOW_LENGTH = 4  # Number of most recent frames to produce the input to the network (WINDOW LENGTH)\n",
    "\n",
    "NUM_STEPS = 6_000_005\n",
    "\n",
    "EXPLORATION_STEPS = 4_500_005  # Number of steps over which the initial value # of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "\n",
    "OBSERVE = 20_000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400_000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10_000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by optimizer\n",
    "GAMMA = 0.99  # Discount factor\n",
    "\n",
    "SAVE_INTERVAL = 200_000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 7  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env.n_foods = 1\n",
    "    env.grid_size = [20, 20]\n",
    "    env.unit_size = 10\n",
    "    env.unit_gap = 1\n",
    "    env.random_init = True\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Size:  [20 20]\nImage shape before pre-processing:  (200, 200, 3)\nImage shape after pre-processing:  (84, 84)\nNo. of actions:  4\n\nBefore Preprocessing:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAfter Preprocessing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n/home/ankur/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAjklEQVR4nO3YsQ3CQBAF0TGiE4sAQUtErsIhkUXmHnAN7oOAxNVQghdpL2CZi0dPup+cdN1K/jk0MEX/Gj3G0xGYstFXuCy56e3MnI4+w2XJTYcrj3R02zjFypKbBu8OdD58oqKioqKioqKioqKioqKiooXQL76QAN7AJRudgGW3KrvpvQXah6rf2bQJ+gEcoQuYn2k7LgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x7F896CAD7B00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = get_env()\n",
    "x_t = env.reset()\n",
    "\n",
    "# Controller\n",
    "game_controller = env.controller\n",
    "# Grid\n",
    "grid_object = game_controller.grid\n",
    "grid_pixels = grid_object.grid\n",
    "print('Grid Size: ', grid_object.grid_size)\n",
    "# Snake(s)\n",
    "snakes_array = game_controller.snakes\n",
    "snake_object1 = snakes_array[0]\n",
    "\n",
    "state_shape = grid_pixels.shape\n",
    "print('Image shape before pre-processing: ', state_shape)\n",
    "print('Image shape after pre-processing: ', (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "nb_actions = env.action_space.n\n",
    "print('No. of actions: ', nb_actions)\n",
    "\n",
    "print('\\nBefore Preprocessing:')\n",
    "env.render()\n",
    "\n",
    "print('\\nAfter Preprocessing:')\n",
    "\n",
    "processed_image = np.uint8(resize(rgb2gray(x_t), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "img = Image.fromarray(processed_image)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE:  (4, 84, 84)\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\npermute_1 (Permute)          (None, 84, 84, 4)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 20, 20, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 9, 9, 64)          0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               1606144   \n_________________________________________________________________\nactivation_4 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 2052      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 4)                 0         \n=================================================================\nTotal params: 1,686,180\nTrainable params: 1,686,180\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE:  (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "\n",
    "def r(raw):\n",
    "    raw = [f.reshape((1,) + INPUT_SHAPE) for f in raw]\n",
    "    raw = np.asarray(raw).reshape(input_shape)\n",
    "    raw = np.expand_dims(raw, axis=0)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def process_observation(observation):\n",
    "    assert observation.ndim == 3  # (height, width, channel)\n",
    "    img = Image.fromarray(observation)\n",
    "    img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "    processed_observation = np.array(img)\n",
    "    assert processed_observation.shape == INPUT_SHAPE\n",
    "    return processed_observation.astype('uint8') / 255.  # saves storage in experience memory\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        # (width, height, channels)\n",
    "        model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "    elif K.image_dim_ordering() == 'th':\n",
    "        # (channels, width, height)\n",
    "        model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "    else:\n",
    "        raise RuntimeError('Unknown image_dim_ordering.')\n",
    "    print('INPUT SHAPE: ', input_shape)\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.compile(loss=huber_loss, optimizer=Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "\n",
    "target_model = get_model()\n",
    "target_model.compile(loss=huber_loss, optimizer=Adam(lr=LEARNING_RATE), metrics=['mae'])\n",
    "\n",
    "\n",
    "def get_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(env.action_space.n)\n",
    "    if random.random() < 0.5:\n",
    "        act_values = model.predict(r(state))\n",
    "    else:\n",
    "        act_values = target_model.predict(r(state))\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "def get_test_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(env.action_space.n)\n",
    "    act_values = model.predict(r(state))\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "def update_model():\n",
    "    minibatch = random.sample(memory, BATCH_SIZE)\n",
    "    total_loss = 0.\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            best_action = np.argmax(model.predict(r(next_state))[0])\n",
    "            target = (reward + GAMMA * target_model.predict(r(next_state))[0][best_action])\n",
    "        target_f = model.predict(r(state))\n",
    "        target_f[0][action] = target\n",
    "        h = model.fit(r(state), target_f, epochs=1, verbose=0)\n",
    "        total_loss += h.history['loss'][0]\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def update_target():\n",
    "    minibatch = random.sample(memory, BATCH_SIZE)\n",
    "    total_loss = 0.\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            best_action = np.argmax(target_model.predict(r(next_state))[0])\n",
    "            target = (reward + GAMMA * model.predict(r(next_state))[0][best_action])\n",
    "        target_f = target_model.predict(r(state))\n",
    "        target_f[0][action] = target\n",
    "        h = model.fit(r(state), target_f, epochs=1, verbose=0)\n",
    "        total_loss += h.history['loss'][0]\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def replay():\n",
    "    if random.random() > 0.5:\n",
    "        return update_target()\n",
    "    else:\n",
    "        return update_model()\n",
    "\n",
    "\n",
    "def setup_summary():\n",
    "    episode_total_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Total_Reward/Episode', episode_total_reward)\n",
    "    episode_avg_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Average_Max_Q/Episode', episode_avg_max_q)\n",
    "    episode_duration = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n",
    "    episode_avg_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Average_Loss/Episode', episode_avg_loss)\n",
    "    epsilon = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Epsilon/Episode', epsilon)\n",
    "    curr_time_step = tf.Variable(0.)\n",
    "    tf.summary.scalar(ENV_NAME + '/Steps/Episode', curr_time_step)\n",
    "\n",
    "    summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss, epsilon,\n",
    "                    curr_time_step]\n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return summary_placeholders, update_ops, summary_op\n",
    "# def get_initial_state(self, observation):\n",
    "#     processed_observation = np.uint8(resize(rgb2gray(observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "#     state = [processed_observation for _ in range(WINDOW_LENGTH)]\n",
    "#     return np.stack(state, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      1 / TIMESTEP:        9 / DURATION:    10 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      2 / TIMESTEP:       66 / DURATION:    57 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:      3 / TIMESTEP:      100 / DURATION:    34 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      4 / TIMESTEP:      122 / DURATION:    22 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      5 / TIMESTEP:      171 / DURATION:    49 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:      6 / TIMESTEP:      198 / DURATION:    27 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      7 / TIMESTEP:      239 / DURATION:    41 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:      8 / TIMESTEP:      264 / DURATION:    25 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      9 / TIMESTEP:      439 / DURATION:   175 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     10 / TIMESTEP:      577 / DURATION:   138 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:     11 / TIMESTEP:      602 / DURATION:    25 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     12 / TIMESTEP:      632 / DURATION:    30 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     13 / TIMESTEP:      761 / DURATION:   129 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:     14 / TIMESTEP:      777 / DURATION:    16 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:     15 / TIMESTEP:      785 / DURATION:     8 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     16 / TIMESTEP:      826 / DURATION:    41 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     17 / TIMESTEP:      884 / DURATION:    58 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:     18 / TIMESTEP:      892 / DURATION:     8 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\nEPISODE:     19 / TIMESTEP:      923 / DURATION:    31 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     20 / TIMESTEP:      952 / DURATION:    29 / EPSILON: 1.00000 / TOTAL_REWARD:  -1 / AVG_MAX_Q: 3.0000 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3b87016f0d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mduration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtotal_q_max\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "memory = deque(maxlen=NUM_REPLAY_MEMORY)\n",
    "state = deque(maxlen=WINDOW_LENGTH)\n",
    "\n",
    "env = get_env()\n",
    "observation = env.reset()\n",
    "observation = process_observation(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "state.append(observation)\n",
    "\n",
    "current = [observation for _ in range(WINDOW_LENGTH)]\n",
    "previous = [observation for _ in range(WINDOW_LENGTH)]\n",
    "\n",
    "# Parameters used for summary\n",
    "total_reward = 0.\n",
    "total_q_max = 0.\n",
    "total_loss = 0.\n",
    "duration = 0\n",
    "episode = 0\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "summary_placeholders, update_ops, summary_op = setup_summary()\n",
    "summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, sess.graph)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "epsilon = INITIAL_EPSILON\n",
    "\n",
    "for t in range(NUM_STEPS):\n",
    "    action = get_action(current, epsilon)\n",
    "\n",
    "    observation, reward, terminal, _ = env.step(int(action))\n",
    "    observation = process_observation(observation)\n",
    "    total_reward += reward\n",
    "    duration += 1\n",
    "\n",
    "    total_q_max += np.argmax(model.predict(r(state)))\n",
    "    state.append(observation)\n",
    "\n",
    "    # Remember\n",
    "    current = list(state)\n",
    "    memory.append((previous, action, reward, current, terminal))\n",
    "    previous = current\n",
    "\n",
    "    if t > OBSERVE:\n",
    "        # Train\n",
    "        if t % TRAIN_INTERVAL == 0:\n",
    "            total_loss += replay()\n",
    "        \n",
    "        # Save Weights Interval\n",
    "        if t % SAVE_INTERVAL == 0:\n",
    "            model.save_weights((SAVE_NETWORK_PATH + '/snake1_{}.h5').format(t))\n",
    "    \n",
    "        # Update Target Network\n",
    "        if t % TARGET_UPDATE_INTERVAL == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "        \n",
    "        # Exploration Rate Decay\n",
    "        if epsilon > FINAL_EPSILON:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "\n",
    "    # End of episode\n",
    "    if terminal:\n",
    "        # Writing Summary Log\n",
    "        avg_q = total_q_max / float(duration)\n",
    "        avg_loss = total_loss / (float(duration) / float(TRAIN_INTERVAL))\n",
    "        if t >= OBSERVE:\n",
    "            stats = [total_reward, avg_q, duration, avg_loss, float(epsilon), float(t)]\n",
    "\n",
    "            for i in range(len(stats)):\n",
    "                sess.run(update_ops[i], feed_dict={\n",
    "                    summary_placeholders[i]: float(stats[i])\n",
    "                })\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, episode + 1)\n",
    "\n",
    "        # Debug\n",
    "        if t < OBSERVE:\n",
    "            mode = 'random'\n",
    "        elif OBSERVE <= t < OBSERVE + EXPLORATION_STEPS:\n",
    "            mode = 'explore'\n",
    "        else:\n",
    "            mode = 'exploit'\n",
    "        print(\n",
    "            'EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} /'\n",
    "            ' EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / '\n",
    "            'AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                episode + 1, t, duration, epsilon,\n",
    "                total_reward, avg_q, avg_loss, mode))\n",
    "\n",
    "        total_reward = 0.\n",
    "        total_q_max = 0.\n",
    "        total_loss = 0.\n",
    "        duration = 0\n",
    "        episode += 1\n",
    "\n",
    "        observation = env.reset()\n",
    "        for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "            observation, _, terminal, _ = env.step(env.action_space.sample())  # Do nothing\n",
    "            if terminal:\n",
    "                observation = env.reset()\n",
    "                break\n",
    "        observation = process_observation(observation)\n",
    "        state = deque(maxlen=WINDOW_LENGTH)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        state.append(observation)\n",
    "        current = [observation for i in range(4)]\n",
    "        previous = [observation for i in range(4)]\n",
    "\n",
    "model.save_weights(SAVE_NETWORK_PATH + '/snake1_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 1/10 [00:01<00:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 2/10 [00:01<00:08,  1.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 3/10 [00:01<00:05,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 96\n2.0 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 4/10 [00:02<00:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 5/10 [00:04<00:05,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 6/10 [00:05<00:04,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 7/10 [00:06<00:02,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 8/10 [00:06<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 9/10 [00:07<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 10/10 [00:08<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights('snake1 ddqn/snake1_ddqn.h5f')\n",
    "%matplotlib\n",
    "for e in tqdm(range(10)):\n",
    "    state = deque(maxlen=WINDOW_LENGTH)\n",
    "    env = get_env()\n",
    "    observation = env.reset()\n",
    "    observation = process_observation(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "    state.append(observation)\n",
    "\n",
    "    current = [observation for _ in range(WINDOW_LENGTH)]\n",
    "\n",
    "    total_reward = 0.\n",
    "    duration = 0\n",
    "    episode = 0\n",
    "\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = get_test_action(current, 0.)\n",
    "        # env.render()\n",
    "        observation, reward, terminal, _ = env.step(int(action))\n",
    "        observation = process_observation(observation)\n",
    "        total_reward += reward\n",
    "        duration += 1\n",
    "\n",
    "        state.append(observation)\n",
    "\n",
    "        # Remember\n",
    "        current = list(state)\n",
    "\n",
    "        if duration >= 600:\n",
    "            print(duration)\n",
    "            break\n",
    "\n",
    "    print(total_reward, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
