{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_snake\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Flatten, Dense, Permute, Activation\n",
    "import keras.backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Size:  [25 25]\n",
      "Image shape before pre-processing:  (250, 250, 3)\n",
      "Image shape after pre-processing:  (84, 84)\n",
      "No. of actions:  4\n",
      "\n",
      "Before Preprocessing:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Preprocessing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/ankur/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAc0lEQVR4nO3SsQ2AIBSE4cM4AWvQ2DqCrsFq9nbswjYsYHEkWED+qy9f3iUvFI3P9oMJOgu6u8VberP0ON0F50vmdi05P53+AXYx1hqGoz2x518dB9jo4RY100uBgoKCgoKCgoKCgoKCgoKCgoKCgoJ+pwFUBQaymeRnTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x7F872E8DD710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV_NAME = 'snake-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network (WINDOW LENGTH)\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "env.n_foods = 1\n",
    "env.grid_size = [25, 25]\n",
    "env.unit_size = 10\n",
    "env.unit_gap = 1\n",
    "env.random_init = True\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "x_t = env.reset()\n",
    "\n",
    "# Controller\n",
    "game_controller = env.controller\n",
    "# Grid\n",
    "grid_object = game_controller.grid\n",
    "grid_pixels = grid_object.grid\n",
    "print('Grid Size: ', grid_object.grid_size)\n",
    "# Snake(s)\n",
    "snakes_array = game_controller.snakes\n",
    "snake_object1 = snakes_array[0]\n",
    "\n",
    "state_shape = grid_pixels.shape\n",
    "print('Image shape before pre-processing: ', state_shape)\n",
    "print('Image shape after pre-processing: ', (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "nb_actions = env.action_space.n\n",
    "print('No. of actions: ', nb_actions)\n",
    "\n",
    "print('\\nBefore Preprocessing:')\n",
    "env.render()\n",
    "\n",
    "print('\\nAfter Preprocessing:')\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "processed_image = np.uint8(resize(rgb2gray(x_t), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "img = Image.fromarray(processed_image)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 10_000_005\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EXPLORATION_STEPS = 7_000_000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "INITIAL_REPLAY_SIZE = 20_000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400_000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 7000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "MOMENTUM = 0.95  # Momentum used by RMSProp\n",
    "MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "SAVE_INTERVAL = 500_000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 10  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "\n",
    "LOAD_NETWORK = False\n",
    "\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\n",
    "NUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "        self.t = 0\n",
    "\n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, self.q_network = self.build_network()\n",
    "        print(self.q_network.summary())\n",
    "        q_network_weights = self.q_network.trainable_weights\n",
    "\n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_network()\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in\n",
    "                                      range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n",
    "\n",
    "        if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "            os.makedirs(SAVE_NETWORK_PATH)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Load network\n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "\n",
    "    def build_network(self):\n",
    "        input_shape = (STATE_LENGTH,) + (FRAME_WIDTH, FRAME_HEIGHT)\n",
    "        model = Sequential()\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            # (width, height, channels)\n",
    "            model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "        elif K.image_dim_ordering() == 'th':\n",
    "            # (channels, width, height)\n",
    "            model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "        else:\n",
    "            raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "        model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "        model.add(Activation('linear'))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "    def get_initial_state(self, observation, last_observation):\n",
    "        processed_observation = np.maximum(observation, last_observation)\n",
    "        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "        state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "        return np.stack(state, axis=0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        # Anneal epsilon linearly over time\n",
    "        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run(self, state, action, reward, terminal, observation):\n",
    "        next_state = np.append(state[1:, :, :], observation, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        if self.t >= INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if self.t % TRAIN_INTERVAL == 0:\n",
    "                self.train_network()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=self.t)\n",
    "                print('Successfully saved: ' + save_path)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Write summary\n",
    "            if self.t >= INITIAL_REPLAY_SIZE:\n",
    "                stats = [self.total_reward, self.total_q_max / float(self.duration),\n",
    "                         self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), float(self.t)]\n",
    "                for i in range(len(stats)):\n",
    "                    self.sess.run(self.update_ops[i], feed_dict={\n",
    "                        self.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.sess.run(self.summary_op)\n",
    "                self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "\n",
    "            # Debug\n",
    "            if self.t < INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print(\n",
    "                'EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                    self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "                    self.total_reward, self.total_q_max / float(self.duration),\n",
    "                    self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def train_network(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        for data in minibatch:\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(\n",
    "            feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Total_Reward/Episode', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Average_Max_Q/Episode', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Average_Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        custom_steps = tf.Variable(0.)\n",
    "        tf.summary.scalar(ENV_NAME + '/Steps/Episode', custom_steps)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss, custom_steps]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "    def load_network(self):\n",
    "        print(SAVE_NETWORK_PATH)\n",
    "        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Training new network...')\n",
    "\n",
    "    def load_weights(self):\n",
    "        self.q_network.load_weights('model_weights.h5')\n",
    "        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        self.q_values = self.q_network(s)\n",
    "\n",
    "    def get_action_at_test(self, state):\n",
    "        if random.random() <= 0.05:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(observation, last_observation):\n",
    "    processed_observation = np.maximum(observation, last_observation)\n",
    "    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "    return np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_7 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(num_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "observation = env.reset()\n",
    "state = agent.get_initial_state(observation, observation)\n",
    "for _ in tqdm_notebook(range(NUM_STEPS)):\n",
    "    last_observation = observation\n",
    "    action = agent.get_action(state)\n",
    "    observation, reward, terminal, _ = env.step(int(action))\n",
    "    processed_observation = preprocess(observation, last_observation)\n",
    "    state = agent.run(state, action, reward, terminal, processed_observation)\n",
    "\n",
    "    if terminal:\n",
    "        observation = env.reset()\n",
    "        for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "            last_observation = observation\n",
    "            observation, _, terminal, _ = env.step(env.action_space.sample())  # Do nothing\n",
    "            if terminal:\n",
    "                observation = env.reset()\n",
    "                break\n",
    "        state = agent.get_initial_state(observation, observation)\n",
    "\n",
    "agent.q_network.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 -1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 -1 0 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 1\n",
      "0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 0\n",
      "0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    terminal = False\n",
    "    observation = env.reset()\n",
    "    state = agent.get_initial_state(observation, observation)\n",
    "    while not terminal:\n",
    "        last_observation = observation\n",
    "        action = agent.get_action_at_test(state)\n",
    "        observation, reward, terminal, _ = env.step(int(action))\n",
    "        processed_observation = preprocess(observation, last_observation)\n",
    "        state = np.append(state[1:, :, :], processed_observation, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
